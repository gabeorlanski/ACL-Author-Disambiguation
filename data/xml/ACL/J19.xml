<?xml version='1.0' encoding='UTF-8'?>
<collection id="J19">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 45, Issue 1 - March 2019</booktitle>
      <month>March</month>
      <year>2019</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <title>Unsupervised Compositionality Prediction of Nominal Compounds</title>
      <author><first>Silvio</first><last>Cordeiro</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Marco</first><last>Idiart</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <doi>10.1162/coli_a_00341</doi>
      <abstract>Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.</abstract>
      <pages>1–57</pages>
      <url>J19-1001</url>
    </paper>
    <paper id="2">
      <title>Learning an Executable Neural Semantic Parser</title>
      <author><first>Jianpeng</first><last>Cheng</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Vijay</first><last>Saraswat</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/coli_a_00342</doi>
      <abstract>This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.</abstract>
      <pages>59–94</pages>
      <url>J19-1002</url>
    </paper>
    <paper id="3">
      <title>Parsing <fixed-case>C</fixed-case>hinese Sentences with Grammatical Relations</title>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Yufei</first><last>Chen</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Meichun</first><last>Liu</last></author>
      <doi>10.1162/coli_a_00343</doi>
      <abstract>We report our work on building linguistic resources and data-driven parsers in the grammatical relation (GR) analysis for Mandarin Chinese. Chinese, as an analytic language, encodes grammatical information in a highly configurational rather than morphological way. Accordingly, it is possible and reasonable to represent almost all grammatical relations as bilexical dependencies. In this work, we propose to represent grammatical information using general directed dependency graphs. Both only-local and rich long-distance dependencies are explicitly represented. To create high-quality annotations, we take advantage of an existing TreeBank, namely, Chinese TreeBank (CTB), which is grounded on the Government and Binding theory. We define a set of linguistic rules to explore CTB’s implicit phrase structural information and build deep dependency graphs. The reliability of this linguistically motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, data-driven, including graph- and transition-based, models are explored for Chinese GR parsing. For graph-based parsing, a new perspective, graph merging, is proposed for building flexible dependency graphs: constructing complex graphs via constructing simple subgraphs. Two key problems are discussed in this perspective: (1) how to decompose a complex graph into simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph. For transition-based parsing, we introduce a neural parser based on a list-based transition system. We also discuss several other key problems, including dynamic oracle and beam search for neural transition-based parsing. Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models. The empirical analysis suggests several directions for future study.</abstract>
      <pages>95–136</pages>
      <url>J19-1003</url>
    </paper>
    <paper id="4">
      <title>Automatic Inference of Sound Correspondence Patterns across Multiple Languages</title>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <doi>10.1162/coli_a_00344</doi>
      <abstract>Sound correspondence patterns play a crucial role for linguistic reconstruction. Linguists use them to prove language relationship, to reconstruct proto-forms, and for classical phylogenetic reconstruction based on shared innovations. Cognate words that fail to conform with expected patterns can further point to various kinds of exceptions in sound change, such as analogy or assimilation of frequent words. Here I present an automatic method for the inference of sound correspondence patterns across multiple languages based on a network approach. The core idea is to represent all columns in aligned cognate sets as nodes in a network with edges representing the degree of compatibility between the nodes. The task of inferring all compatible correspondence sets can then be handled as the well-known minimum clique cover problem in graph theory, which essentially seeks to split the graph into the smallest number of cliques in which each node is represented by exactly one clique. The resulting partitions represent all correspondence patterns that can be inferred for a given data set. By excluding those patterns that occur in only a few cognate sets, the core of regularly recurring sound correspondences can be inferred. Based on this idea, the article presents a method for automatic correspondence pattern recognition, which is implemented as part of a Python library which supplements the article. To illustrate the usefulness of the method, I present how the inferred patterns can be used to predict words that have not been observed before.</abstract>
      <pages>137–161</pages>
      <url>J19-1004</url>
    </paper>
    <paper id="5">
      <title>A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Chen</first><last>Xing</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <doi>10.1162/coli_a_00345</doi>
      <abstract>We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task involves matching a response candidate with a conversation context, the challenges for which include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching. This motivates us to propose a new matching framework that can sufficiently carry important information in contexts to matching and model relationships among utterances at the same time. The new framework, which we call a sequential matching framework (SMF), lets each utterance in a context interact with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network (RNN) that models relationships among utterances. Context-response matching is then calculated with the hidden states of the RNN. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experiment results show that both models can significantly outperform state-of-the-art matching methods. We also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage important information in contexts for matching.</abstract>
      <pages>163–197</pages>
      <url>J19-1005</url>
    </paper>
  </volume>
</collection>
